---
title: JAX
---

# [JAX](https://github.com/google/jax)

[Differential Programming with JAX course](https://ericmjl.github.io/dl-workshop/) is nice. [Meta Optimal Transport](https://github.com/facebookresearch/meta-ot) is nice JAX repo to run/study.

[Robert Lange](https://github.com/RobertTLange) has nice JAX repos.

## Notes

- [JAX is more like NumPy with Autograd, Jit, and TPU/GPU support](https://twitter.com/rasbt/status/1538903583365681152)

## Links

- [audax](https://github.com/SarthakYadav/audax) - Home for audio ML in JAX. Has common features, learnable frontends, pretrained supervised and self-supervised models.
- [tinygp](https://github.com/dfm/tinygp) - Extremely lightweight library for building Gaussian Process models in Python, built on top of jax.
- [GPJax](https://github.com/thomaspinder/GPJax) - Didactic Gaussian process package for researchers in Jax.
- [Mctx](https://github.com/deepmind/mctx) - Monte Carlo tree search in JAX.
- [Pipelined Swarm Training](https://github.com/kingoflolz/swarm-jax) - Swarm training framework using Haiku + JAX + Ray for layer parallel transformer language models on unreliable, heterogeneous nodes.
- [JAX MuZero](https://github.com/Hwhitetooth/jax_muzero) - JAX implementation of the MuZero agent.
- [Jax Influence](https://github.com/google-research/jax-influence) - Scalable implementation of Influence Functions in JaX.
- [BlackJAX](https://github.com/blackjax-devs/blackjax) - Library of samplers for JAX that works on CPU as well as GPU. ([Twitter](https://twitter.com/blackjax_mcmc))
- [GPax](https://github.com/google-research/gpax) - Jax/Flax codebase for Gaussian processes including meta and multi-task Gaussian processes.
- [jax-fenics-adjoint](https://github.com/IvanYashchuk/jax-fenics-adjoint) - Differentiable interface to FEniCS/Firedrake for JAX using dolfin-adjoint/pyadjoint.
- [jax-ekf](https://github.com/brentyi/jax-ekf) - Generic EKF, with support for non-Euclidean manifolds.
- [PaLM - Jax](https://github.com/lucidrains/PaLM-jax) - Implementation of the specific Transformer architecture from PaLM - Scaling Language Modeling with Pathways - in Jax.
- [Pre-trained image classification models for Jax/Haiku](https://github.com/abarcel/haikumodels)
- [Flaxformer: transformer architectures in JAX/Flax](https://github.com/google/flaxformer)
- [KFAC-JAX - Second Order Optimization with Approximate Curvature in JAX](https://github.com/deepmind/kfac-jax)
- [flowjax](https://github.com/danielward27/flowjax) - Normalizing flow implementations in jax.
- [Jax3D](https://github.com/google-research/jax3d) - Library for neural rendering in Jax and aims to be a nimble NeRF ecosystem.
- [DALL·E 2 in JAX](https://github.com/lucidrains/DALLE2-jax)
- [JAXNS](https://github.com/Joshuaalbert/jaxns) - Nested sampling in JAX.
- [AUX](https://github.com/deepmind/dm_aux) - Audio processing library in JAX, for JAX.
- [Nice DeepMind Jax libraries](https://twitter.com/DeepMind/status/1517146462571794433)
- [Machine Learning with JAX - From Zero to Hero (2021)](https://www.youtube.com/playlist?list=PLBoQnSflObckOARbMK9Lt98Id0AKcZurq)
- [Flax](https://github.com/google/flax) - Neural network library for JAX designed for flexibility. ([Docs](https://flax.readthedocs.io/en/latest/))
- [JAX talks by HuggingFace](https://www.youtube.com/playlist?list=PLo2EIpI_JMQtQrEduYXbRz4X50mTiOi8S)
- [Homomorphic Encryption in JAX](https://github.com/nkandpa2/he_jax)
- [JAX implementation of Learning to learn by gradient descent by gradient descent](https://github.com/teddykoker/learning-to-learn-jax)
- [Normalizing Flows in JAX](https://github.com/ChrisWaites/jax-flows)
- [Big Vision](https://github.com/google-research/big_vision) - Designed for training large-scale vision models on Cloud TPU VMs. Based on Jax/Flax libraries.
- [Jax vs. Julia (Vs PyTorch) (2022)](https://kidger.site/thoughts/jax-vs-julia/) ([HN](https://news.ycombinator.com/item?id=31263516))
- [minGPT in JAX](https://github.com/mgrankin/minGPT)
- [flaxvision](https://github.com/rolandgvc/flaxvision) - Selection of neural network models ported from torchvision for JAX & Flax.
- [JAX version of clip guided diffusion scripts](https://github.com/nshepperd/jax-guided-diffusion)
- [Functorch](https://github.com/pytorch/functorch) - Jax-like composable function transforms for PyTorch. ([HN](https://news.ycombinator.com/item?id=31424588))
- [Ninjax](https://github.com/danijar/ninjax) - Module system for JAX that offers full state access and allows to easily combine modules from other libraries.
- [Functional Transformer](https://github.com/awf/functional-transformer) - Pure-functional implementation of a machine learning transformer model in Python/JAX.
- [JAX + Units](https://github.com/dfm/jpu) - Provides and interface between JAX and Pint to allow JAX to support operations with units.
- [Infinite Recommendation Networks (∞-AE) in JAX](https://github.com/noveens/infinite_ae_cf)
- [Differential Programming with JAX course](https://ericmjl.github.io/dl-workshop/) ([Code](https://github.com/ericmjl/dl-workshop))
- [Algorithms for Privacy-Preserving Machine Learning in JAX](https://github.com/deepmind/jax_privacy)
- [Connex](https://github.com/leonard-gleyzer/connex) - Small JAX library built on Equinox whose aim is to incorporate artificial analogues of biological neural network attributes into deep learning research and architecture design.
- [Rax](https://github.com/google/rax) - Composable Learning to Rank using JAX.
- [JaX is faster than PyTorch but harder to debug](https://twitter.com/kevin_zakka/status/1538634474107314176)
- [JAX Meta Learning](https://github.com/tristandeleu/jax-meta-learning) - Collection of meta-learning algorithms in JAX.
- [Gymnax](https://github.com/RobertTLange/gymnax) - RL Environments in JAX.
- [Pax](https://github.com/google/paxml) - Framework to configure and run machine learning experiments on top of Jax.
- [SymPy2Jax](https://github.com/google/sympy2jax) - Turn SymPy expressions into trainable JAX expressions.
- [JAX Typing](https://github.com/google/jaxtyping) - Type annotations and runtime checking for shape and dtype of JAX arrays, and PyTrees.
- [CoDeX](https://github.com/google/codex) - Data compression in JAX.
- [DiBS](https://github.com/larslorch/dibs) - Python JAX implementation for DiBS, fully differentiable method for joint Bayesian inference of the DAG and parameters of general, causal Bayesian networks.
- [Generative Adversarial Networks in JAX](https://github.com/lweitkamp/GANs-JAX)
- [Neural implicit queries](https://github.com/nmwsharp/neural-implicit-queries) - Perform geometric queries on neural implicit surfaces like ray casting, intersection testing, fast mesh extraction, closest points, and more.
- [CLIP-JAX](https://github.com/borisdayma/clip-jax) - Train CLIP models using JAX and transformers.
- [BLOOM Inference in JAX](https://github.com/huggingface/bloom-jax-inference)
